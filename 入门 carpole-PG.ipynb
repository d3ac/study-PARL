{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d3ac/miniconda3/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n",
      "/tmp/ipykernel_103136/1077153389.py:88: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343998658/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  softmax_input = self.network.forward(torch.FloatTensor(self.ep_obs).to(device))\n",
      "/home/d3ac/miniconda3/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: 18.9\n",
      "episode:  100 Evaluation Average Reward: 74.9\n",
      "episode:  200 Evaluation Average Reward: 275.2\n",
      "episode:  300 Evaluation Average Reward: 300.0\n",
      "episode:  400 Evaluation Average Reward: 300.0\n",
      "episode:  500 Evaluation Average Reward: 300.0\n",
      "episode:  600 Evaluation Average Reward: 300.0\n",
      "episode:  700 Evaluation Average Reward: 300.0\n",
      "episode:  800 Evaluation Average Reward: 300.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    147\u001b[0m     time_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 148\u001b[0m     main()\n\u001b[1;32m    149\u001b[0m     time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    150\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe total time is \u001b[39m\u001b[39m'\u001b[39m, time_end \u001b[39m-\u001b[39m time_start)\n",
      "Cell \u001b[0;32mIn[1], line 121\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39m# Train\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[39m# 只采一盘？N个完整序列\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(STEP):\n\u001b[0;32m--> 121\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mchoose_action(state)  \u001b[39m# softmax概率选择action\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     next_state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    123\u001b[0m     agent\u001b[39m.\u001b[39mstore_transition(state, action, reward)   \u001b[39m# 新函数 存取这个transition\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 56\u001b[0m, in \u001b[0;36mPG.choose_action\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchoose_action\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m---> 56\u001b[0m     observation \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mFloatTensor(observation)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     57\u001b[0m     network_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mforward(observation)\n\u001b[1;32m     58\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@ Author: Peter Xiao\n",
    "@ Date: 2020.7.20\n",
    "@ Filename: PG.py\n",
    "@ Brief: 使用 蒙特卡洛策略梯度Reinforce训练CartPole-v0\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Hyper Parameters for PG Network\n",
    "GAMMA = 0.95  # discount factor\n",
    "LR = 0.01  # learning rate\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.backends.cudnn.enabled = False  # 非确定性算法\n",
    "\n",
    "\n",
    "class PGNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PGNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, state_dim * 10)\n",
    "        self.fc2 = nn.Linear(state_dim * 10, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.fc1(x))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PG(object):\n",
    "    # dqn Agent\n",
    "    def __init__(self, env):  # 初始化\n",
    "        # 状态空间和动作空间的维度\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        # init N Monte Carlo transitions in one game\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        # init network parameters\n",
    "        self.network = PGNetwork(state_dim=self.state_dim, action_dim=self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=LR)\n",
    "\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = torch.FloatTensor(observation).to(device)\n",
    "        network_output = self.network.forward(observation)\n",
    "        with torch.no_grad():\n",
    "            prob_weights = F.softmax(network_output, dim=0).cuda().data.cpu().numpy()\n",
    "        # prob_weights = F.softmax(network_output, dim=0).detach().numpy()\n",
    "        action = np.random.choice(range(prob_weights.shape[0]),\n",
    "                                  p=prob_weights)  # select action w.r.t the actions prob\n",
    "        return action\n",
    "\n",
    "    # 将状态，动作，奖励这一个transition保存到三个列表中\n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Step 1: 计算每一步的状态价值\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        # 注意这里是从后往前算的，所以式子还不太一样。算出每一步的状态价值\n",
    "        # 前面的价值的计算可以利用后面的价值作为中间结果，简化计算；从前往后也可以\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * GAMMA + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)  # 减均值\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)  # 除以标准差\n",
    "        discounted_ep_rs = torch.FloatTensor(discounted_ep_rs).to(device)\n",
    "\n",
    "        # Step 2: 前向传播\n",
    "        softmax_input = self.network.forward(torch.FloatTensor(self.ep_obs).to(device))\n",
    "        # all_act_prob = F.softmax(softmax_input, dim=0).detach().numpy()\n",
    "        neg_log_prob = F.cross_entropy(input=softmax_input, target=torch.LongTensor(self.ep_as).to(device), reduction='none')\n",
    "\n",
    "        # Step 3: 反向传播\n",
    "        loss = torch.mean(neg_log_prob * discounted_ep_rs)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 每次学习完后清空数组\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Hyper Parameters\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 1000  # Episode limitation\n",
    "STEP = 300  # Step limitation in an episode\n",
    "TEST = 10  # The number of experiment test every 100 episode\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    env = gym.make(ENV_NAME)\n",
    "    agent = PG(env)\n",
    "\n",
    "    for episode in range(EPISODE):\n",
    "        # initialize task\n",
    "        state, info = env.reset()\n",
    "        # Train\n",
    "        # 只采一盘？N个完整序列\n",
    "        for step in range(STEP):\n",
    "            action = agent.choose_action(state)  # softmax概率选择action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.store_transition(state, action, reward)   # 新函数 存取这个transition\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # print(\"stick for \",step, \" steps\")\n",
    "                agent.learn()   # 更新策略网络\n",
    "                break\n",
    "\n",
    "        # Test every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(TEST):\n",
    "                state, info = env.reset()\n",
    "                for j in range(STEP):\n",
    "                    env.render()\n",
    "                    action = agent.choose_action(state)  # direct action for test\n",
    "                    state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "            ave_reward = total_reward/TEST\n",
    "            print ('episode: ', episode, 'Evaluation Average Reward:', ave_reward)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.time()\n",
    "    main()\n",
    "    time_end = time.time()\n",
    "    print('The total time is ', time_end - time_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d3ac/miniconda3/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 Evaluation Average Reward: 23.6\n",
      "episode:  100 Evaluation Average Reward: 20.2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@ Author: Peter Xiao\n",
    "@ Date: 2020.7.20\n",
    "@ Filename: PG.py\n",
    "@ Brief: 使用 蒙特卡洛策略梯度Reinforce训练CartPole-v0\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "# Hyper Parameters for PG Network\n",
    "GAMMA = 0.95  # discount factor\n",
    "LR = 0.01  # learning rate\n",
    "\n",
    "# Use GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.backends.cudnn.enabled = False  # 非确定性算法\n",
    "\n",
    "\n",
    "class PGNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PGNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, state_dim * 10)\n",
    "        self.fc2 = nn.Linear(state_dim * 10, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.tanh(self.fc1(x))\n",
    "        out = F.softmax(self.fc2(out), dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PG(object):\n",
    "    # dqn Agent\n",
    "    def __init__(self, env):  # 初始化\n",
    "        # 状态空间和动作空间的维度\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        # init N Monte Carlo transitions in one game\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "        # init network parameters\n",
    "        self.network = PGNetwork(state_dim=self.state_dim, action_dim=self.action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=LR)\n",
    "\n",
    "        # init some parameters\n",
    "        self.time_step = 0\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = torch.FloatTensor(observation).to(device)\n",
    "        network_output = self.network.forward(observation)\n",
    "        with torch.no_grad():\n",
    "            prob_weights = F.softmax(network_output, dim=0).cuda().data.cpu().numpy()\n",
    "        # prob_weights = F.softmax(network_output, dim=0).detach().numpy()\n",
    "        action = np.random.choice(range(prob_weights.shape[0]),\n",
    "                                  p=prob_weights)  # select action w.r.t the actions prob\n",
    "        return action\n",
    "\n",
    "    # 将状态，动作，奖励这一个transition保存到三个列表中\n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "\n",
    "    def learn(self):\n",
    "        self.time_step += 1\n",
    "\n",
    "        # Step 1: 计算每一步的状态价值\n",
    "        discounted_ep_rs = np.zeros_like(self.ep_rs)\n",
    "        running_add = 0\n",
    "        # 注意这里是从后往前算的，所以式子还不太一样。算出每一步的状态价值\n",
    "        # 前面的价值的计算可以利用后面的价值作为中间结果，简化计算；从前往后也可以\n",
    "        for t in reversed(range(0, len(self.ep_rs))):\n",
    "            running_add = running_add * GAMMA + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = running_add\n",
    "\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)  # 减均值\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)  # 除以标准差\n",
    "        discounted_ep_rs = torch.FloatTensor(discounted_ep_rs).to(device)\n",
    "\n",
    "        # Step 2: 前向传播\n",
    "        softmax_input = self.network.forward(torch.FloatTensor(self.ep_obs).to(device))\n",
    "        # all_act_prob = F.softmax(softmax_input, dim=0).detach().numpy()\n",
    "        neg_log_prob = F.cross_entropy(input=softmax_input, target=torch.LongTensor(self.ep_as).to(device), reduction='none')\n",
    "\n",
    "        # Step 3: 反向传播\n",
    "        loss = torch.mean(neg_log_prob * discounted_ep_rs)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # 每次学习完后清空数组\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Hyper Parameters\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "EPISODE = 1000  # Episode limitation\n",
    "STEP = 300  # Step limitation in an episode\n",
    "TEST = 10  # The number of experiment test every 100 episode\n",
    "\n",
    "\n",
    "def main():\n",
    "    # initialize OpenAI Gym env and dqn agent\n",
    "    env = gym.make(ENV_NAME)\n",
    "    agent = PG(env)\n",
    "\n",
    "    for episode in range(EPISODE):\n",
    "        # initialize task\n",
    "        state, info = env.reset()\n",
    "        # Train\n",
    "        # 只采一盘？N个完整序列\n",
    "        for step in range(STEP):\n",
    "            action = agent.choose_action(state)  # softmax概率选择action\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            agent.store_transition(state, action, reward)   # 新函数 存取这个transition\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # print(\"stick for \",step, \" steps\")\n",
    "                agent.learn()   # 更新策略网络\n",
    "                break\n",
    "\n",
    "        # Test every 100 episodes\n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(TEST):\n",
    "                env1 = gym.make(ENV_NAME, render_mode='human')\n",
    "                state, info = env1.reset()\n",
    "                for j in range(STEP):\n",
    "                    env1.render()\n",
    "                    action = agent.choose_action(state)  # direct action for test\n",
    "                    state, reward, done, _, _ = env1.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "            ave_reward = total_reward/TEST\n",
    "            print ('episode: ', episode, 'Evaluation Average Reward:', ave_reward)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    time_start = time.time()\n",
    "    main()\n",
    "    time_end = time.time()\n",
    "    print('The total time is ', time_end - time_start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d3ac/miniconda3/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
